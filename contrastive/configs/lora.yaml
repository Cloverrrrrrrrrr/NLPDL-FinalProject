model_name: meta-llama/Meta-Llama-3-8B-Instruct
data_path: train_data/data/llama_error.jsonl
output_dir: contrastive/lora
learning_rate: 0.00001
per_device_train_batch_size: 3
um_train_epochs: 2
weight_decay: 0.01
optim: adamw_torch
fp16: False
bf16: True
gradient_accumulation_steps: 4
use_lora: True
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05